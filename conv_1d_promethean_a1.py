# -*- coding: utf-8 -*-
"""Conv 1D Promethean-A1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rdBNuCteWC4pBYb_nPfro2Akb-rGtBGK
"""

import os
from os import listdir
from os.path import join
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


import tensorflow as tf

import time


x_train = np.load("x_train.npy")
y_train=np.load("y_train.npy")

train_data = tf.convert_to_tensor(x_train)

train_labels = tf.convert_to_tensor(y_train)


train_dataset=tf.data.Dataset.from_tensor_slices((train_data, train_labels))


train_dataset = train_dataset.batch(1)


model = tf.keras.models.Sequential([
    tf.keras.layers.InputLayer(input_shape=(10)),
    tf.keras.layers.Reshape((10,1)),
    tf.keras.layers.Conv1D(32, kernel_size=(3), activation="relu", padding="valid"),
    tf.keras.layers.Conv1D(8, kernel_size=(3),activation="relu", padding="valid"),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(32, activation="relu"),
    tf.keras.layers.Dense(8, activation="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid")
])

model.summary()

lr = 1e-2
opt = tf.keras.optimizers.Adam(lr)

loss_fn=tf.keras.losses.BinaryCrossentropy()
model.compile(loss='binary_crossentropy', optimizer=opt)



epochs = 20
for epoch in range(epochs):
    print("\nStart of epoch %d" % (epoch,))

    # Iterate over the batches of the dataset.
    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        # print(step)

        # Open a GradientTape to record the operations run
        # during the forward pass, which enables auto-differentiation.
        with tf.GradientTape() as tape:

            # Run the forward pass of the layer.
            # The operations that the layer applies
            # to its inputs are going to be recorded
            # on the GradientTape.
            t0=time.time()
            
            logits = model(x_batch_train, training=True)  # Logits for this minibatch

            t1=time.time()

            # Compute the loss value for this minibatch.
            loss_value = loss_fn(y_batch_train, logits)

            t2=time.time()

        # Use the gradient tape to automatically retrieve
        # the gradients of the trainable variables with respect to the loss.
        grads = tape.gradient(loss_value, model.trainable_weights)

        t3=time.time()

        # Run one step of gradient descent by updating
        # the value of the variables to minimize the loss.
        opt.apply_gradients(zip(grads, model.trainable_weights))

        t4=time.time()

        # Log every 200 batches.
        if step % 1 == 0:
            print("Fwprop time %d: %.4f",(step, float(t1-t0)))
            print("gradient calculation time %d: %.4f",(step, float(t3-t2)))
            print("loss time %d: %.4f",(step, float(t2-t1)))
            print("weight update time %d: %.4f",(step, float(t4-t3)))
